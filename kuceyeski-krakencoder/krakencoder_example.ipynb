{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krakencoder usage example\n",
    "\n",
    "This notebook provides an example of how to load connectome data and apply a pretrained Krakencoder model to that data.\n",
    "\n",
    "The process is as follows:\n",
    "1.   Load model\n",
    "2.   Load new data and do mild domain adaptation (map input data mean to training data mean)\n",
    "3.   Transform each input data flavor into 128-dimensional latent space\n",
    "4.   Average latent space across all types (\"fusion\") or across only SC data (\"fusionSC\")\n",
    "  * Note: this might be used for prediction, clustering, etc.\n",
    "5.   Transform \"fusion\" averaged latent vectors to output connectomes\n",
    "\n",
    "For more information, visit [github.com/kjamison/krakencoder](https://github.com/kjamison/krakencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from krakencoder.model import Krakencoder\n",
    "from krakencoder.adaptermodel import KrakenAdapter\n",
    "from krakencoder.utils import square2tri, tri2square, numpyvar\n",
    "from krakencoder.data import generate_adapt_transformer, load_transformers_from_file\n",
    "from krakencoder.fetch import fetch_model_data\n",
    "from krakencoder import loss\n",
    "\n",
    "import kraken_jupyter_functions as kjf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=\"/home/jovyan/shared/krakencoder/model_data/\"\n",
    "\n",
    "# load model checkpoint and precomputed PCA transforms\n",
    "# The first time these are used, they are downloaded to package_dir/model_data (~1.3GB)\n",
    "# To use an alternate storage location, you can set the KRAKENCODER_DATA environment variable\n",
    "checkpoint_file=fetch_model_data('kraken_chkpt_SCFC_fs86+shen268+coco439_pc256_225paths_latent128_20240413_ep002000.pt', \n",
    "                                 data_folder=data_folder)\n",
    "\n",
    "ioxfm_file_list=fetch_model_data(['kraken_ioxfm_SCFC_fs86_pc256_710train.npy',\n",
    "                                  'kraken_ioxfm_SCFC_shen268_pc256_710train.npy',\n",
    "                                  'kraken_ioxfm_SCFC_coco439_pc256_710train.npy'], \n",
    "                                  data_folder=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_net, checkpoint_info = Krakencoder.load_checkpoint(checkpoint_file, eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_list, transformer_info_list = load_transformers_from_file(ioxfm_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new model that wraps the inner kraken model and includes PCA transforms from raw data\n",
    "net=KrakenAdapter(inner_model=inner_net,\n",
    "                  data_transformer_list=[transformer_list[conntype] for conntype in checkpoint_info['input_name_list']],\n",
    "                  linear_polynomial_order=0,\n",
    "                  eval_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load connectivity data files\n",
    "* Data should be a BIDS(ish) .zip file with a set of ROIxROI .tsv files for each connectome\n",
    "* File must contain a `participants_info.tsv` with a row for each subject, including BIDS `participant_id`, etc...\n",
    "* Each tsv file should be `sub-<subject>_atlas-<atlas>_meas-<flavor>_relmat.dense.tsv`\n",
    "    * where `<atlas>` could be `fs86`, `shen268`, `coco439`, etc...\n",
    "    * and `<flavor>` could be `FCcorrHPF`, `FCcorrHPFGSR`, `FCpcorrHPF`, `SCsdstreamVN`, `SCifod2actVN`, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load data\n",
    "\n",
    "conndata_squaremats = None\n",
    "participants_info = None\n",
    "\n",
    "kjf.jupyter_create_upload_widget(\n",
    "    loadfile_callback=lambda filename, filebytes=None, allowed_extensions=None: kjf.callback_load_and_process_data(\n",
    "        filename,\n",
    "        filebytes,\n",
    "        allowed_extensions,\n",
    "        variablename_loaddata=\"conndata_squaremats\",\n",
    "        variablename_participants=\"participants_info\",\n",
    "        globals_set=globals()\n",
    "    ),\n",
    "    extensions=[\".zip\", \".mat\"],\n",
    "    multiple=False,\n",
    "    initial_tab=\"local\",\n",
    "    initial_local_path=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conndata_squaremats is None:\n",
    "    # in case we use \"Run All\" and it tries to run the next cell before the data is loaded\n",
    "    kjf.callback_load_and_process_data(\n",
    "        os.path.abspath(\"./example_bidsdir_5flav_20subj.zip\"),\n",
    "        variablename_loaddata=\"conndata_squaremats\",\n",
    "        variablename_participants=\"participants_info\",\n",
    "        globals_set=globals(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data are lists of [roi x roi] square matrices so convert those to [subj x edges]\n",
    "\n",
    "#skip internal header keys\n",
    "conntypes=[c for c in conndata_squaremats.keys() if not c.startswith(\"_\")]\n",
    "\n",
    "conndata={}\n",
    "conndata_triidx={} #store so we can restore square later\n",
    "for c in conntypes:\n",
    "    conndata[c]={'data': np.stack([square2tri(x) for x in conndata_squaremats[c]])}\n",
    "    _, conndata_triidx[c]=square2tri(conndata_squaremats[c][0],return_indices=True)\n",
    "    print(\"conndata_squaremats['%s']=%s\" % (c,kjf.data_shape_string(conndata_squaremats[c])),end='')\n",
    "    print(\" -> conndata['%s']['data']=%s\" % (c,kjf.data_shape_string(conndata[c]['data'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SIMPLE domain adaptation transform: map mean(input subjects) to mean(training subjects) for each flavor\n",
    "# and transform each input data flavor to match training data mean\n",
    "#\n",
    "# It's not really needed for these data since they are from HCP-YA already, which the model was trained on.\n",
    "# That is why the model fits are all roughly \"y = 1.0*x + 0\". It is shown here for demonstration purposes only.\n",
    "adxfm_dict={}\n",
    "conndata_adapted={}\n",
    "for c in conndata:\n",
    "  print(\"Adapting %s\" % (c))\n",
    "  adxfm_dict[c]=generate_adapt_transformer(input_data=conndata[c]['data'],\n",
    "                                           target_data=transformer_info_list[c],\n",
    "                                           adapt_mode='meanfit+meanshift')\n",
    "  conndata_adapted[c]={'data':adxfm_dict[c].transform(conndata[c]['data'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input data to krakencoder latent space\n",
    "encoded_data={}\n",
    "\n",
    "#loop through all of the input names from the saved checkpoint,\n",
    "# because the encoder/decoder indices are in this order.\n",
    "for encidx, c in enumerate(checkpoint_info['input_name_list']):\n",
    "  if not c in conndata_adapted:\n",
    "    #if this input type was not in the example data, skip it\n",
    "    continue\n",
    "  with torch.no_grad():\n",
    "    encoded_data[c]=net(conndata_adapted[c]['data'],encoder_index=encidx, decoder_index=-1)\n",
    "\n",
    "# compute average latent representation across SC input types\n",
    "fusion_type='fusionSC'\n",
    "\n",
    "if fusion_type=='fusion':\n",
    "  encoded_fusion=torch.mean(torch.stack([encoded_data[c] for c in encoded_data]),axis=0)\n",
    "elif fusion_type=='fusionSC':\n",
    "  encoded_fusion=torch.mean(torch.stack([encoded_data[c] for c in encoded_data if c.startswith('SC')]),axis=0)\n",
    "\n",
    "print(\"%s latent space representation: \" % (fusion_type), kjf.data_shape_string(encoded_fusion))\n",
    "\n",
    "# Now predict output connectomes from fusion latent representation\n",
    "# Predictions are stored in predicted_alltypes[inputtype][outputtype]\n",
    "predicted_alltypes={fusion_type:{}}\n",
    "\n",
    "for decidx, c in enumerate(checkpoint_info['input_name_list']):\n",
    "  with torch.no_grad():\n",
    "    _,pred=net(encoded_fusion,encoder_index=-1, decoder_index=decidx)\n",
    "  predicted_alltypes[fusion_type][c]=numpyvar(pred) #convert back to numpy for analysis\n",
    "  print(\"predicted_alltypes['%s']['%s']: \" % (fusion_type,c), kjf.data_shape_string(predicted_alltypes[fusion_type][c]))\n",
    "\n",
    "#add fusion latent representation to output\n",
    "predicted_alltypes[fusion_type]['encoded']=numpyvar(encoded_fusion)\n",
    "print(\"predicted_alltypes['%s']['%s']: \" % (fusion_type,'encoded'), kjf.data_shape_string(predicted_alltypes[fusion_type]['encoded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick visualization of an observed and predicted connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert upper tri back to square to display observed and predicted connectomes\n",
    "\n",
    "conntype='FCcorr_fs86_hpf'\n",
    "\n",
    "isubj=0\n",
    "\n",
    "Cobs_square=tri2square(conndata_adapted[conntype]['data'][isubj,:],\n",
    "                            tri_indices=conndata_triidx[conntype],\n",
    "                            diagval=1) #diagval=1 for FC\n",
    "Cpred_square=tri2square(predicted_alltypes[fusion_type][conntype][isubj,:],\n",
    "                            tri_indices=conndata_triidx[conntype],\n",
    "                            diagval=1) #diagval=1 for FC\n",
    "\n",
    "#make sure these are converted back to CPU/numpy before trying to display\n",
    "Cobs_square=numpyvar(Cobs_square)\n",
    "Cpred_square=numpyvar(Cpred_square)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "im=plt.imshow(Cobs_square, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "plt.colorbar(im,fraction=0.046, pad=.04)\n",
    "plt.title('Obs. %s: Subj %s' % (conntype,participants_info.iloc[isubj]['subject']))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "im=plt.imshow(Cpred_square, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "plt.colorbar(im,fraction=0.046, pad=0.04)\n",
    "plt.title('Pred. %s: Subj %s' % (conntype,participants_info.iloc[isubj]['subject']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of prediction identifiability\n",
    "* For each flavor, we look at (SUBJECT)x(SUBJECT) correlations after removing population mean for\n",
    "    1. Observed vs Observed (measured connectome variability)\n",
    "    2. Predicted vs Predicted (predicted connetome variability)\n",
    "    3. Observed vs Predicted (prediction accuracy)\n",
    "* For Observed vs Predicted, we show the rank for each observed connectome (row) against the set of all predicted connectomes\n",
    "    * For a perfectly identifiable set of predictions, the rank would be 1 for every row\n",
    "    * \"Average Rank Percentile\" is the average of these ranks, expressed as a percentile (1.0=perfect, 0.5=chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input=fusion_type\n",
    "\n",
    "nperm=10000\n",
    "#nperm=100 #for quick testing\n",
    "\n",
    "for conntype in conntypes:\n",
    "    Xobs=conndata[conntype]['data']\n",
    "    Xpred=predicted_alltypes[prediction_input][conntype]\n",
    "\n",
    "    Xobs_mean=np.mean(Xobs,axis=0)\n",
    "\n",
    "    # Remove the group mean before computing similarity\n",
    "    Xobs=Xobs-Xobs_mean\n",
    "    Xpred=Xpred-Xobs_mean\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2,ncols=2,figsize=(10,10))\n",
    "    axs=axs.flatten()\n",
    "\n",
    "    for iax, (x1, x2, group1, group2) in enumerate(\n",
    "        [\n",
    "            [Xobs, Xobs, \"Observed\", \"Observed\"],\n",
    "            [Xpred, Xpred, \"Predicted\", \"Predicted\"],\n",
    "            [Xobs, Xpred, \"Observed\", \"Predicted\"],\n",
    "        ]\n",
    "    ):\n",
    "        sim=loss.xycorr(x1,x2)\n",
    "        avgrank=loss.corravgrank(cc=sim)\n",
    "        \n",
    "        ax=axs[iax]\n",
    "        im=ax.imshow(sim, vmin=-1,vmax=1,cmap='RdBu_r')\n",
    "        # im=ax.imshow(sim, cmap='RdBu_r')\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        groupxy=[group2,group1]\n",
    "        titlestr='%s vs %s' % (groupxy[1],groupxy[0])\n",
    "        \n",
    "        if [group1,group2]==[\"Observed\", \"Predicted\"]:\n",
    "            #for Observed vs Predicted, display lines and rank for each observation\n",
    "            ranklist=[]\n",
    "            for i in range(sim.shape[0]):\n",
    "                cci=sim[i,:] #get row for this observation\n",
    "                cci=(cci-np.nanmin(cci))/(np.nanmax(cci)-np.nanmin(cci)) #scale row from 0-1\n",
    "                cci=(cci-.5)*.8 #scale for display\n",
    "                \n",
    "                cci_maxidx=np.argmax(cci)\n",
    "                cci_sortidx=np.argsort(np.argsort(cci)[::-1])\n",
    "                ax.plot(np.arange(len(cci)),i-cci,color='k',linewidth=.5)\n",
    "                ax.plot([0,len(cci)-1],[i-cci[cci_maxidx], i-cci[cci_maxidx]],'k--',linewidth=.25)\n",
    "                ax.plot(cci_maxidx,i-cci[cci_maxidx],'k.',markerfacecolor='none')\n",
    "                ranklist.append(cci_sortidx[i]+1)\n",
    "            \n",
    "            avgrank_index=np.mean(ranklist)\n",
    "            titlestr+='\\n'+'Avg Rank %.1f, %%ile: %.3f' % (avgrank_index,avgrank)\n",
    "            ax.set_yticks(np.arange(sim.shape[0]))\n",
    "            ax.set_yticklabels(['%d' % (r) for r in ranklist])\n",
    "            ax.tick_params(axis='y',length=0)\n",
    "            \n",
    "        ax.set_title(titlestr)\n",
    "        ax.set_xlabel(groupxy[0])\n",
    "        ax.set_ylabel(groupxy[1])\n",
    "\n",
    "        #add colorbar that is the same height as the plot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    \n",
    "    #plot permutation test histogram for avgrank\n",
    "    ax=axs[-1]\n",
    "\n",
    "    avgrank_obs2pred_perm = [\n",
    "        loss.corravgrank(\n",
    "            cc=loss.xycorr(Xobs, Xpred[np.random.permutation(Xpred.shape[0]), :])\n",
    "        )\n",
    "        for i in range(nperm)\n",
    "    ]\n",
    "\n",
    "    p_perm=np.mean(avgrank<avgrank_obs2pred_perm)\n",
    "    xh=ax.hist(avgrank_obs2pred_perm,bins=20,alpha=.5)\n",
    "    ax.plot([avgrank,avgrank],[0,max(xh[0])],'-r')\n",
    "    ax.legend(['test=%.3f' % (avgrank),'%d perm' % (nperm)])\n",
    "    ax.set_xlim([0,1.05])\n",
    "    ax.set_aspect(aspect=.9*np.diff(ax.get_xlim())/np.diff(ax.get_ylim()))\n",
    "    ax.set_xlabel('Avg Rank %%ile')\n",
    "    ax.set_title('Avg Rank Significance\\n$p_{perm}\\leq$%g' % (p_perm))\n",
    "\n",
    "    fig.subplots_adjust(wspace=.5,hspace=0)\n",
    "\n",
    "    fig.suptitle(r'%s $\\rightarrow$ %s' % (prediction_input,conntype))\n",
    "    fig.patch.set_linewidth(1)\n",
    "    fig.patch.set_edgecolor('k')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted connectomes back to square matrices\n",
    "\n",
    "predicted_square={}\n",
    "for conntype in conntypes:\n",
    "    if conntype == 'encoded':\n",
    "        continue\n",
    "    diagval = 1 if conntype.startswith(\"FC\") else 0\n",
    "    \n",
    "    predicted_square[conntype] = [\n",
    "        tri2square(\n",
    "            predicted_alltypes[fusion_type][conntype][i, :],\n",
    "            tri_indices=conndata_triidx[conntype],\n",
    "            diagval=diagval,\n",
    "        )\n",
    "        for i in range(predicted_alltypes[fusion_type][conntype].shape[0])\n",
    "    ]\n",
    "    print(\"predicted_alltypes['%s']['%s']=%s\" % (fusion_type,conntype,kjf.data_shape_string(predicted_alltypes[fusion_type][conntype])),end='')\n",
    "    print(\" -> predicted_square['%s']=%s\" % (conntype,kjf.data_shape_string(predicted_square[conntype])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predicted connectomes to a new BIDS(ish) .zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save data\n",
    "\n",
    "numflav = len(predicted_square)\n",
    "numsubj = len(predicted_square[conntypes[0]])\n",
    "\n",
    "outfile_default = \"output_bidsdir_%skraken_%dflav_%dsubj.zip\" % (\n",
    "    fusion_type,\n",
    "    numflav,\n",
    "    numsubj,\n",
    ")\n",
    "\n",
    "kjf.jupyter_create_save_widget(\n",
    "    savefile_callback=lambda variablename, filename: kjf.callback_saveoutput(\n",
    "        variablename,\n",
    "        filename,\n",
    "        bids_desc=fusion_type + \"kraken\",\n",
    "        variablename_participants=\"participants_info\",\n",
    "        globals_set=globals()\n",
    "    ),\n",
    "    outvariable_default=\"predicted_square\",\n",
    "    outfile_default=outfile_default,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krakenexact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

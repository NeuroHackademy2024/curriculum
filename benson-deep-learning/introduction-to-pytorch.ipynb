{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is one of the preeminent machine-learning and optimization libraries currently available. It contains a number of powerful features that drastically simplify the task of fitting models and training neural networks. While we won't have time in this tutorial to examine more than a few of the core features, there are many additional tutorials available online. This tutorial will roughly follow a few of the introductory PyTorch tutorials available at [pytorch.org/tutorials](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be covering three topics, each briefly:\n",
    "1. The basic structure of objects in PyTorch\n",
    "2. Non-linear optimization\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Noah C. Benson &lt;[nben@uw.edu](mailto:nben@uw.edu)&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic PyTorch Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, PyTorch appears to be somewhat like NumPy in that it gives the user a set of classes and functions for interacting with a `Tensor` type that behaves much like NumPy's `ndarray` type. Both NumPy and PyTorch, for example, define functions like `log`, `sin`, and `mean` that work with their respective array type. However, the `Tensor` and `ndarray` objects aren't interchangeable. This is because PyTorch `Tensor`s are intended for use in optimization problems, and thus they track a wide variety of data about what computations they have been used in. These data are critical for performing efficient gradient-descent parameter-tuning, which is generally required for optimization and for training neural networks.\n",
    "\n",
    "Let's start by importing a number of PyTorch modules for use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library:\n",
    "import torch\n",
    "# Import the Neural-Network sub-module of PyTorch:\n",
    "from torch import nn\n",
    "# The DataLoader class is a helper for loading datasets during model training:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Finally, we want to import matplotlib.pyplot so that we can visualize the\n",
    "# results of our training.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of PyTorch revolves around a single datatype, the PyTorch `Tensor` (`torch.Tensor`). At first glance, the tensor class seems almost identical to NumPy's `ndarray` class. However, although they share many similarities, the two types are not directly compatible. The main difference between them is that tensors track a variety of data that can be used to calculate gradients. Let's take a look at the `torch.Tensor` type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torch library uses many of the same conventions as the numpy library.\n",
    "u = torch.zeros((10, 3))  # Make a 10x3 matrix.\n",
    "\n",
    "u[:,0] = 1 # Assign the first column to have values of 1.\n",
    "\n",
    "u = (u - 0.5) *  3.0 # Subtract 1/2 then multiply the tensor by 3.\n",
    "\n",
    "print(u) # Print the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we will sometimes get an error if we try to use numpy arrays and\n",
    "# tensors interchangeably. Although some operations like this may work, you\n",
    "# should generally work with one or the other and not try to mix them.\n",
    "import numpy as np\n",
    "\n",
    "a = np.zeros((10,3))\n",
    "\n",
    "torch.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, if you need to extract a numpy array from a tensor, you can\n",
    "# use first detach the tensor (i.e., remove it from the backend system\n",
    "# that keeps track of gradients) then request the numpy representation.\n",
    "a = u.detach().numpy()\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the really great things about PyTorch is that it can easily find the gradient of a particular calculation. Let's look at this via an example in which we use PyTorch to calculate the point along a 2D curve that is closest to the point `(0,0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start with, let's define the curve itself; we define this as a\n",
    "# parametric curve s(t) = (x(t), y(t)):\n",
    "def curve(t):\n",
    "    x = torch.sin(t) - 0.5\n",
    "    y = torch.cos(t)*2 + torch.cos(2*t) + 0.75\n",
    "    return (x,y)\n",
    "\n",
    "# Let's see what this curve looks like:\n",
    "t = torch.linspace(0, np.pi*2, 100)\n",
    "(x,y) = curve(t)\n",
    "\n",
    "# Plot these points as a curve:\n",
    "plt.plot(x, y, 'k-')\n",
    "# Also plot the x and y axes:\n",
    "plt.plot([-3,3], [0,0], 'k-', lw=0.5)\n",
    "plt.plot([0,0], [-3,3], 'k-', lw=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to find the point closest to (0,0). This minimizes the\n",
    "# distance between (0,0) and (x(t), y(t)). Let's define this as the\n",
    "# loss function (i.e., the quantity we are minimizing).\n",
    "def loss(t):\n",
    "    (x,y) = curve(t)\n",
    "    return torch.sqrt((x - 0)**2 + (y - 0)**2)\n",
    "\n",
    "# We can calculate the loss for any value of t:\n",
    "t = torch.tensor(1.5)\n",
    "dist = loss(t)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss function doesn't return a Python floating-point number; rather it returns a PyTorch tensor object containing just a floating point number. This is because, although this value will act like a floating point number, it also contains extra information about how it was calculated. We can, in fact, use this to determine the gradient of `curve(t)` at the point `t`. For this to work, we just need to specify that our input tensor requires that gradient calculations be tracked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(1.5, requires_grad=True)\n",
    "dist = loss(t)\n",
    "\n",
    "# Run the backward gradient routine to calculate the gradients:\n",
    "dist.backward()\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination of calling `output.backward()` then examining the `input.grad` value works not just for single values, but also for high-dimensional tensors, and PyTorch is very efficient about calculating these gradients.\n",
    "\n",
    "Because we can calculate the gradient for our loss function, we can minimize the loss by using a simple gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to start the minimization with t equal to this value:\n",
    "t = torch.tensor(2.1, requires_grad=True)\n",
    "\n",
    "# Declare an optimizer (SGD: stochastic gradient descent).\n",
    "# We are minimizing over the argument t (i.e., the input), and\n",
    "# we provide a low learning-rate (which affects how big the\n",
    "# optimizer's steps are).\n",
    "optimizer = torch.optim.SGD([t], lr=0.05)\n",
    "\n",
    "# Now we can take several steps to see if the optimizer converges.\n",
    "for step_number in range(20): # we'll take 20 steps...\n",
    "    # We're starting a new step, so we reset the gradients.\n",
    "    optimizer.zero_grad()\n",
    "    dist = loss(t)\n",
    "    print(\"Step number\", step_number,\n",
    "          \"  t = \", float(t),\n",
    "          \"  loss = \", float(dist))\n",
    "    dist.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(t, loss(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the solution we found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points that we found in the search above:\n",
    "(x0,y0) = curve(t)\n",
    "# We will need to detach these values to plot them because they were\n",
    "# calculated using t, which requires gradients be tracked.\n",
    "x0 = x0.detach().numpy()\n",
    "y0 = y0.detach().numpy()\n",
    "\n",
    "# Plot all the points in the curve:\n",
    "(x,y) = curve(torch.linspace(0, np.pi*2, 100))\n",
    "plt.plot(x, y, 'k-')\n",
    "# Also plot the x and y axes:\n",
    "plt.plot([-3,3], [0,0], 'k-', lw=0.5)\n",
    "plt.plot([0,0], [-3,3], 'k-', lw=0.5)\n",
    "# And finally plot the point we found.\n",
    "plt.plot(x0, y0, 'r.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, because the optimization is a simple gradient descent, the starting point can potentially change the result. Specifically, the optimization algorithm will typically find the nearest local minimum to the starting point. We used the starting value of `t = 2.1`, which is close enough to the global minimum that we found the global minimum; however, this wouldn't be the case if we started with `t = 0`. With a start point of `t = 0`, we would instead find the local minimum that is close to the `x`-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network to recognize images of handwritten numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this tutorial provides an example walkthrough of how to train a neural network using PyTorch. We will use a public datasetof written numerals, the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), and we will setup a simple neural network that can be trained to recognize the number represented in a small (28x28) image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we'll be using the MNIST dataset, which contains images of handwritten numerals (0-9), each of which has been labeled. The dataset is available on various locations around the internet, but it has been prepopulated on the JupyterHub for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data.\n",
    "(training_labels, training_images) = torch.load(\n",
    "    '/home/jovyan/shared/benson-deep-learning/mnist_train.pt',\n",
    "    weights_only=True)\n",
    "\n",
    "# Load test data.\n",
    "(test_labels, test_images) = torch.load(\n",
    "    '/home/jovyan/shared/benson-deep-learning/mnist_test.pt',\n",
    "    weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a representation of this dataset in the variables `training_labels`/`trainig_image` and `test_labels`/`test_images`. Naturally, these will correspond to our training and testing datasets; these are independent in order to avoid overfitting.\n",
    "\n",
    "To use these with PyTorch, we will need to convert them into PyTorch `Dataset` classes. Datasets simply contain a number of data-points (images of numerals in our case) and paired with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images.float()\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, ii):\n",
    "        return (self.images[ii], self.labels[ii])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "training_data = MNISTDataset(training_images, training_labels)\n",
    "test_data = MNISTDataset(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch typically expects to interact with trainig and testing data via a class called `DataLoader`, which we imported earlier, from the `torch.utils.data` module. This class manages the loading and caching of individual data samples from our datasets, and can even perform loading in parallel if the dataset is large and cumbersome enough. In this cell, we setup data loaders for the two datasets and demonstrate how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data usually arrives to the model-fitting routine as \n",
    "# a batch of samples. This sets the size of each batch.\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# \n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [Batch-Size, Image-Channels, Height, Width]: \")\n",
    "    print(\"     \", X.shape, X.dtype)\n",
    "    print(\"Shape of y [1 Label per Batch]: \")\n",
    "    print(\"     \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we loop over the `test_dataloader` object, it yields a sequence of `(X,y)` tuples where `X` is a tensor of clothing images from the Fashion MNIST dataset and `y` is a list of the corresponding integer labels. Notice that the first dimension of both `X` and `y` is 64, which is also our training batch size--i.e., the dataloaders always yield data samples in batches. Notice also that the second dimension of the `X` value is 1; this is because the images are grayscale and thus have only one color-channel. The final two dimensions are the height and width of the image.\n",
    "\n",
    "The `y` labels that are returned are just integers, 0-9, corresponding to the associated numeral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can verify that these labels are correct by looking at a few of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll and label 4 images in a 2x2 grid:\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(4,4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get one batch of samples from the dataloader--remember that this\n",
    "# batch will have 64 images in it.\n",
    "for (X_batch,y_batch) in train_dataloader:\n",
    "    # Go ahead and plot the first four of these images\n",
    "    for (ax,X,y) in zip(axs, X_batch, y_batch):\n",
    "        ax.imshow(X)\n",
    "        ax.set_title(y.item())\n",
    "        ax.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step for our neural network project is to define the neural network itself.\n",
    "\n",
    "PyTorch makes defining neural network models fairly easy. You simply need to declare a Python class that inherits from the `torch.nn.Module` class, which represents a single module of a neural network. Because a module can itself contain a number of network layers, the module can either represent a piece of a network or it can be the entire neural network.\n",
    "\n",
    "When declaring a `Module`, we need to make sure to define the stack of layers in the network in the class's constructor (the `__init__()` method), and we need to declare how the model is calculated in the class's `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this network is a stack of a few operators. First of all, we have a layer that flattens the inputs from 28x28 images into 784-element vectors. Next, we have a sub-stack of layers that consists of three [linear operators](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), each of which is rectified by a [rectified linear unit](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n",
    "\n",
    "We aren't going to worry too much about what these particular layers of the network are doing. Suffice to say that these are fairly common components of neural networks, and that a full discussion of common neural network layers is beyond the scope of this tutorial.\n",
    "\n",
    "One other thing to note is that the `out_features` of the final linear operator in the stack is 10. This means that the output is in fact a 10-dimensional tensor (like a numpy array whose shape is `(10,)`). Typically when performing this kind of classification problem we interpret each of the output dimensions as representing the likelihood of an input to the model belonging to one of the dataset classes, so if the output features for a particular image are `[0, 0.1, 0, 0.01, 0.5, 0.3, 0.2, 0, 0,3]`, we interpret the model as predicting that the image belongs to class 4 (because `output[4]` is 0.5, which is the largest value in the outputs). In short, we can convert the output feature tensor into a predicted class number by taking the `argmax` of the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In order to train the above neural network, we will need to define a loss function that we are trying to minimize. In this case, we can use a builtin loss function called [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). We won't discuss the details of how this loss function works, but it is commonly used with classification problems like the one we are encoding here.\n",
    "\n",
    "We'll also need an optimizer, and in this case we'll use the same optimizer we used above: SGD (stochastic gradient descent).  Because our model was written using the `torch.nn.Module` class, we can get all of the model parameters by calling the `model.parameters()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the training itself, we just need to step through the data-items in the training dataset and provide them each to the optimizer, much like we did in the simpler optimization example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the training dataset.\n",
    "size = len(train_dataloader.dataset)\n",
    "\n",
    "# Walk through each training batch:\n",
    "for (batch_num, (X, y)) in enumerate(train_dataloader):\n",
    "    # Compute prediction:\n",
    "    pred = model(X)\n",
    "    # And compute the loss of that prediction:\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print out a status message every so often.\n",
    "    if batch_num % 100 == 0:\n",
    "        (loss, current) = (loss.item(), batch_num * len(X))\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it appears that the above cell worked, but what did it do? We can see from the printed lines that as the optimization proceeded, the loss decreased. If we want to see how well this fitting procedure worked, we can look at some examples from the test dataset and see how well the trained model performs. For this, we can essentially copy-and-paste the code-block above that we used to look at the initial images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll and label 4 images in a 2x2 grid:\n",
    "(fig, axs) = plt.subplots(2, 2, figsize=(6,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get one batch of samples from the dataloader--remember that this\n",
    "# batch will have 64 images in it.\n",
    "for (X_batch,y_batch) in test_dataloader:\n",
    "    # We want a random set of 4 images each time, so we keep drawing\n",
    "    # new image batches until we randomly draw a number over 0.9:\n",
    "    if np.random.rand() < 0.9: continue\n",
    "    # Go ahead and plot the first four of these images\n",
    "    for (ax,X,y) in zip(axs, X_batch, y_batch):\n",
    "        ax.imshow(X)\n",
    "        # Get the model's prediction for this particular image:\n",
    "        pred = model(X[None,:])\n",
    "        # Convert the predicted tensor into a single label:\n",
    "        label = torch.argmax(pred)\n",
    "        # If that label is equal to y, the network got it right;\n",
    "        # otherwise, it got it wrong!\n",
    "        y_name = str(y.item())\n",
    "        label_name = str(label.item())\n",
    "        ax.set_title(f\"Estimate: {label_name}; True: {y_name}\")\n",
    "        ax.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our network isn't perfect--if you run the above cell many times, you will see that sometimes the network gets the item type correct, and sometimes it gets it wrong. However, you might also nitce that when the network is wrong, it is wrong in a fairly understandable way (for example, 6 might be labeled a 0 or a 2 might be labeled a 7). This shouldn't be too surprising, considering that we trained a fairly small neural network with a simple architecture, but hopefully this example demonstrates the fundamentals of how PyTorch organizes models and networks used for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional PyTorch materials can be found primarily at [pytorch.org](https://pytorch.org/) (note specifically the [Docs](https://pytorch.org/docs/) and [Tutorials](https://pytorch.org/tutorials/) links at the top of the page). You may also want to check out PyTorch's [60-minute blitz video tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be412e9-d51e-4d80-b9dc-a6f6def33a14",
   "metadata": {},
   "source": [
    "# The Human Connectome Project (HCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada3052-c65e-412e-bb2b-b5fc6566f180",
   "metadata": {},
   "source": [
    "* Began in 2009 as an NIH Blueprint Grand Challenge\n",
    "* Goal of collecting an unprecedented dataset of functional and structural connectivity in the human brain \n",
    "* Funded through consortia:\n",
    "  * Harvard/MGH–UCLA (Harvard / Massachusetts General Hospital and U. of California Los Angeles)  \n",
    "    Development of HCP Technology  \n",
    "    PIs: Bruce Rosen and Arthur Toga\n",
    "  * WU–Minn (Washington U. and U. of Minnesota)  \n",
    "    Young Adult HCP  \n",
    "    PIs: David Van Essen & Kamil Ugurbil\n",
    "* Additional follow-up projects:\n",
    "  * Lifespan Connectome: Healthy adults of all ages\n",
    "  * Disease Connectome: Adults with clinical diagnoses\n",
    "* Data are public and coordinated through a single facility: [https://www.humanconnectome.org/](https://www.humanconnectome.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ee74b-c2c3-477d-b672-5b46c0a0728b",
   "metadata": {},
   "source": [
    "## Young Adult Human Connectome\n",
    "\n",
    "In this notebook we will focus on the Young Adult HCP:\n",
    "* ~1,200 subjects, age 22-35\n",
    "* Includes:\n",
    "  * Structural MRI\n",
    "  * Diffusion-weighted Imaging\n",
    "  * Functional Task-evoked and Resting-state MRI\n",
    "  * 7T MRI (~200 subjects)\n",
    "  * Various Behavioral Data\n",
    "  * Genetic Family Data (~300 twin-pairs)\n",
    "  * Resting-state and Task-evoked MEG (~100 subjects)\n",
    "* Extensively pre-processed\n",
    "* HCPpipelines protocol: https://github.com/Washington-University/HCPpipelines  \n",
    "  Based on FreeSurfer with additional features such as MSM alignments across subjects (Multimodal Surface Matching)\n",
    "\n",
    "The Young Adult HCP is available from multiple sources. One of these sources is the website https://www.humanconnectome.org/ where you can register for the dataset and download the data using your web browser. This works fine, but it is generally easier and interact with datasets of this size over S3 because we don't have to download everything and can instead just download the data we are interested in.\n",
    "\n",
    "To connect to the HCP S3 bucket, we will need to have a set of S3 credentials for the HCP in our `~/.aws/credentials` file (the `~` represents your home directory, so on the hub this is `/home/jovyan/.aws/credentials`). This file typically contains AWS credentials, and format information can be found online ([here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)). To obtain the credentials that must be put there, you will need to register with the HCP. Instructions for doing this are below:\n",
    "\n",
    "### Setting up HCP Credentials\n",
    "* The HCP database is free to access, but you must register with the HCP in order to obtain login credentials. To do so, visit the ConnectomeDB page and follow the instructions to create an account (start with the \"Register\" button).\n",
    "* Once you have created an account, you should be able to login to the database and browse the available datasets through their web portal. You may use this interface to access the HCP data, but in the data showcase we will look at how to use some Python libraries (cloudpathlib and neuropythy) to access the data via the HCP Amazon S3 bucket. The official information (from the HCP) on how to do this can be found at this page. Please follow the first few steps, through the creation of the AWS credentials. Without these, Python cannot access the data on your behalf.\n",
    "* Once you have generated your AWS key (“`ACCESS KEY ID`”) and secret (“`SECRET ACCESS KEY`”), you will need to put them in a file in your home directory named `.aws/credentials`; this file follows a format specified by Amazon Web Services. You can produce this file using the following four commands; these commands are given here but without the access key ID or the secret access key spelled out, so make sure to replace the `______________` with your AWS key ID and to replace the `********************` with your AWS secret access key. You do not need to put these in quotes. Be careful to preserve the single quotes (`'`) in the commands! These commands are meant to be run in a JupyterHub terminal, but you can run them in a JupyterHub notebook on the hub by putting an `!` at the front of each of the commands (i.e., `! mkdir ...` and `! echo ...`):  \n",
    "  ```bash\n",
    "  mkdir -p ~/.aws\n",
    "  echo '[hcp]' > ~/.aws/credentials\n",
    "  echo 'aws_access_key_id = ______________' >> ~/.aws/credentials\n",
    "  echo 'aws_secret_access_key = ********************' >> ~/.aws/credentials\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853ee7e-953d-4ba0-8713-0815659f9e5e",
   "metadata": {},
   "source": [
    "### Accessing the HCP Data on S3\n",
    "\n",
    "Once we have setup our credentials, we can simply tell our `S3Path` and `S3Client` object that our credentials are labeled `hcp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94d8f8-94ec-44f8-a694-2b532adfdfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from cloudpathlib import S3Path, S3Client\n",
    "\n",
    "# Make sure that we have a cache path:\n",
    "cache_path = Path('/tmp/cache')\n",
    "if not cache_path.exists():\n",
    "    cache_path.mkdir()\n",
    "\n",
    "hcp_base_path = S3Path(\n",
    "    's3://hcp-openaccess/HCP_1200/',\n",
    "    client=S3Client(\n",
    "        local_cache_dir=cache_path,\n",
    "        profile_name='hcp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6a6d3-b7a2-4c95-8abf-e094c44d7b90",
   "metadata": {},
   "source": [
    "Once we have created the base HCP path, we can go ahead and look at the contents. The HCP uses 6-digit subject IDs, and we can see all of them by listing the contents of the path we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87bd14-ca71-4d69-a0ac-435135245fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_subdirs = list(hcp_base_path.iterdir())\n",
    "# There are about 1200 of these, so we won't show them all, just the first 10:\n",
    "hcp_subdirs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf1d21-faf5-424d-aef0-2f1468f01ab6",
   "metadata": {},
   "source": [
    "Let's look inside one of these subject's directories. We can write a simple function that crawls the entire directory and prints all of the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02cd14-4078-4ff3-af6e-ac41b1ac5b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utilities import crawl\n",
    "\n",
    "# We'll crawl a specific subpath for subject 100610: 100610/T1w/100610;\n",
    "# this is the FreeSurfer subdirectory of the HCPpipelines output.\n",
    "\n",
    "# If you accidentally run this on the hcp_base_path, it will take a very\n",
    "# long time to run!\n",
    "sub100610_freesurfer_path = hcp_base_path / '100610' / 'T1w' / '100610'\n",
    "crawl(sub100610_freesurfer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9de9e1-d91f-4a0a-b412-d39e6be7448c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we want to load one of these files into nibabel, we can convert it\n",
    "# into a filesystem path (cloudpaths cannot just be passed to nibabel).\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "brain_img_path = sub100610_freesurfer_path / 'mri' / 'brain.mgz'\n",
    "brain_img = nib.load(brain_img_path.fspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9717c-3e6b-4f40-84a0-0c751277cf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a slice from the brain:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(brain_img.dataobj[:,150,:], cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
